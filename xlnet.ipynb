{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install git+https://github.com/monkeyhippies/xlnet.git","execution_count":6,"outputs":[{"output_type":"stream","text":"Collecting git+https://github.com/monkeyhippies/xlnet.git\n  Cloning https://github.com/monkeyhippies/xlnet.git to /tmp/pip-req-build-ufszd3oe\n  Running command git clone -q https://github.com/monkeyhippies/xlnet.git /tmp/pip-req-build-ufszd3oe\nRequirement already satisfied (use --upgrade to upgrade): xlnet==0.1 from git+https://github.com/monkeyhippies/xlnet.git in /opt/conda/lib/python3.6/site-packages\nBuilding wheels for collected packages: xlnet\n  Building wheel for xlnet (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-pqgqi6__/wheels/6e/9a/01/d0ca3ef8d5fc22f32dcdfc011fca0c53b65b4086a90eb71f9f\nSuccessfully built xlnet\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget https://storage.googleapis.com/xlnet/released_models/cased_L-24_H-1024_A-16.zip\n!unzip cased_L-24_H-1024_A-16.zip","execution_count":null,"outputs":[{"output_type":"stream","text":"--2019-06-25 22:36:30--  https://storage.googleapis.com/xlnet/released_models/cased_L-24_H-1024_A-16.zip\nResolving storage.googleapis.com (storage.googleapis.com)... 108.177.127.128, 2a00:1450:4013:c07::80\nConnecting to storage.googleapis.com (storage.googleapis.com)|108.177.127.128|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1338042341 (1.2G) [application/zip]\nSaving to: ‘cased_L-24_H-1024_A-16.zip.1’\n\n     cased_L-24_H-1  12%[=>                  ] 163.96M   164MB/s               ","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom sklearn.model_selection import train_test_split\n\nfrom xlnet.run_classifier import FLAGS\nfrom xlnet.model_utils import get_train_op\nfrom xlnet.run_classifier import InputExample\nfrom xlnet.text_processor import TextProcessor\nfrom xlnet.tf_record_utils import write_tf_record_file\nfrom xlnet.input_fn_builders import tf_record_input_fn_builder\nfrom xlnet import xlnet\n\n# some code omitted here...\n# initialize FLAGS\n# Dummy initializer\nLABELS_LIST = None\nNUM_TRAIN_EPOCHS = 1\nWARMUP_PROPORTION = .1\nTEST_SIZE = .2\nFLAGS([\"python\"])\nFLAGS.model_config_path = \"xlnet_cased_L-24_H-1024_A-16/xlnet_config.json\"\nFLAGS.spiece_model_file = \"xlnet_cased_L-24_H-1024_A-16/spiece.model\"\nFLAGS.learning_rate = 2e-5\nFLAGS.train_batch_size = 8\nFLAGS.max_seq_length = 128\nFLAGS.summary_type = \"last\"\nFLAGS.use_summ_proj = True\n# 1804874 is number of rows of original dataset\nFLAGS.train_steps = int((1 - TEST_SIZE) * 1804874 / FLAGS.train_batch_size * NUM_TRAIN_EPOCHS)\nFLAGS.warmup_steps = int(FLAGS.train_steps * WARMUP_PROPORTION)\n\nSAVE_CHECKPOINTS_STEPS = 5000\nSAVE_SUMMARY_STEPS = 1000\nOUTPUT_DIR = \"../\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def write_tf_records(data, filename_suffix):\n    \"\"\"\n    @data is dataframe\n    \"\"\"\n    processor = TextProcessor(\n        FLAGS.spiece_model_file,\n        LABELS_LIST,\n        FLAGS.max_seq_length,\n        FLAGS.uncased\n    )\n\n    input_examples = [\n        InputExample(\n            guid=None,\n            text_a=text,\n            text_b=None,\n            label=None\n        )\n        for text in data[\"comment_text\"].values\n    ]\n\n    features = processor.process(input_examples)\n    del input_examples\n\n    write_tf_record_file(\n        \"toxicity-\" + filename_suffix + \".tfrecord\",\n        features, data[\"target\"].values\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")\ntrain = train.iloc[: 10000]\ntrain, val = train_test_split(train, shuffle=True,\n    test_size=TEST_SIZE, random_state=42)\n\nfor filename_suffix, data in zip([\"train\", \"val\"], [train, val]):\n    write_tf_records(data, filename_suffix)\n    \ndel train, val, data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_input_fn = tf_record_input_fn_builder(\n    [\"toxicity-train.tfrecord\"], FLAGS.max_seq_length, FLAGS.train_batch_size, 1000\n)\n\nval_input_fn = tf_record_input_fn_builder(\n    [\"toxicity-val.tfrecord\"], FLAGS.max_seq_length, FLAGS.train_batch_size, 1000\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(flags, is_predicting, input_ids, input_mask, segment_ids, labels,\n                 num_labels=None):\n    \"\"\"Creates a classification model.\"\"\"\n\n    xlnet_config = xlnet.XLNetConfig(json_path=FLAGS.model_config_path)\n\n    # RunConfig contains hyperparameters that could be different between pretraining and finetuning.\n    run_config = xlnet.create_run_config(\n        is_training=True,\n        is_finetune=True,\n        FLAGS=flags\n    )\n\n    # Construct an XLNet model\n    xlnet_model = xlnet.XLNetModel(\n        xlnet_config=xlnet_config,\n        run_config=run_config,\n        input_ids=tf.transpose(input_ids),\n        seg_ids=tf.transpose(segment_ids),\n        input_mask=tf.transpose(input_mask)\n    )\n\n    # Get a summary of the sequence using the last hidden state\n    output_layer = xlnet_model.get_pooled_out(\n        summary_type=flags.summary_type,\n        use_summ_proj=flags.use_summ_proj\n    )\n\n    with tf.variable_scope(\"loss\"):\n        output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n        logits = slim.fully_connected(\n            output_layer,\n            num_labels or 2,\n            activation_fn=None,\n            scope=\"fine_tune\"\n        )\n\n        y_pred = tf.nn.softmax(logits)\n        if num_labels is None:\n            y_true = tf.concat(\n                [tf.ones_like(labels) - labels, labels],\n                -1\n            )\n        else:\n            y_true = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n        \n        predicted_labels = tf.squeeze(\n            tf.argmax(\n                y_pred, axis=-1, output_type=tf.int32\n            )\n        )\n\n        # If we're predicting, we want predicted labels and the probabiltiies.\n        if is_predicting:\n            return (predicted_labels, y_pred)\n        else:\n\n            # If we're train/eval, compute loss between predicted and actual label\n            #per_example_loss = -tf.reduce_sum(one_hot_labels * y_pred, axis=-1)\n            #class_weights = tf.constant([[1.0, 50.0]])\n            #weights = tf.reduce_sum(one_hot_labels * class_weights, axis=-1)\n            loss = tf.nn.softmax_cross_entropy_with_logits_v2(\n                y_true, logits\n            )\n            #loss = tf.reduce_mean(loss * weights)\n            loss = tf.reduce_mean(loss)\n            return (loss, predicted_labels, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_fn_builder actually creates our model function\n# using the passed parameters for num_labels, learning_rate, etc.\ndef model_fn_builder(flags, num_labels=None):\n    \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n        \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n\n        input_ids = features[\"input_ids\"]\n        input_mask = features[\"input_mask\"]\n        segment_ids = features[\"segment_ids\"]\n        #label_ids = features[\"label_ids\"]\n\n        is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n\n        # TRAIN and EVAL\n        if not is_predicting:\n\n            (loss, predicted_labels, log_probs) = create_model(\n                flags, is_predicting, input_ids, input_mask, segment_ids,\n                labels\n            )\n\n            train_op, _, _ = get_train_op(flags, loss)\n\n            # Calculate evaluation metrics. \n            def metric_fn(label_ids, predicted_labels):\n                accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n                f1_score = tf.contrib.metrics.f1_score(\n                    label_ids,\n                    predicted_labels)\n                auc = tf.metrics.auc(\n                    label_ids,\n                    predicted_labels)\n                recall = tf.metrics.recall(\n                    label_ids,\n                    predicted_labels)\n                precision = tf.metrics.precision(\n                    label_ids,\n                    predicted_labels) \n                true_pos = tf.metrics.true_positives(\n                    label_ids,\n                    predicted_labels)\n                true_neg = tf.metrics.true_negatives(\n                    label_ids,\n                    predicted_labels)   \n                false_pos = tf.metrics.false_positives(\n                    label_ids,\n                    predicted_labels)  \n                false_neg = tf.metrics.false_negatives(\n                    label_ids,\n                    predicted_labels)\n                return {\n                    \"eval_accuracy\": accuracy,\n                    \"f1_score\": f1_score,\n                    \"auc\": auc,\n                    \"precision\": precision,\n                    \"recall\": recall,\n                    \"true_positives\": true_pos,\n                    \"true_negatives\": true_neg,\n                    \"false_positives\": false_pos,\n                    \"false_negatives\": false_neg\n                }\n\n            eval_metrics = metric_fn(labels, predicted_labels)\n\n            if mode == tf.estimator.ModeKeys.TRAIN:\n                return tf.estimator.EstimatorSpec(\n                    mode=mode,\n                    loss=loss,\n                    train_op=train_op)\n            else:\n                return tf.estimator.EstimatorSpec(\n                    mode=mode,\n                    loss=loss,\n                    eval_metric_ops=eval_metrics)\n        else:\n            (predicted_labels, log_probs) = create_model(\n                flags, is_predicting, input_ids, input_mask,\n                segment_ids, labels\n            )\n\n            predictions = {\n                'probabilities': log_probs,\n                'labels': predicted_labels\n            }\n            return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n\n    # Return the actual model function in the closure\n    return model_fn\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Specify outpit directory and number of checkpoint steps to save\nrun_config = tf.estimator.RunConfig(\n    model_dir=OUTPUT_DIR,\n    save_summary_steps=SAVE_SUMMARY_STEPS,\n    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS\n)\nmodel_fn = model_fn_builder(FLAGS)\n\nestimator = tf.estimator.Estimator(\n  model_fn=model_fn,\n  config=run_config,\n  params={\"batch_size\": FLAGS.train_batch_size}\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(1, 11):\n    steps = i * 5000\n    print('Beginning Training! max_step: {}'.format(steps))\n    current_time = datetime.now()\n    estimator.train(input_fn=train_input_fn, max_steps=steps)\n    print(\"Training took time \", datetime.now() - current_time)\n    estimator.evaluate(input_fn=val_input_fn, steps=5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}