{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# https://arxiv.org/pdf/1508.06576.pdf\n# https://riptutorial.com/keras/example/32608/transfer-learning-using-keras-and-vgg\n!pip install -q tensorflow-gpu==2.0.0-beta1\n\nimport numpy as np\nimport tensorflow as tf\nimport IPython.display as display\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nmpl.rcParams['figure.figsize'] = (12,12)\nmpl.rcParams['axes.grid'] = False\n\nCONTENT_LAYER = \"block2_conv2\"\nVARIATION_LOSS_WEIGHT = tf.constant(1e8, dtype=tf.float32)\nSTYLE_LAYERS = [\n    \"block1_conv2\", \"block2_conv2\",\n    \"block3_conv3\", \"block4_conv3\",\n    \"block5_conv3\"\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"content_path = tf.keras.utils.get_file(\n    'turtle.jpg',\n    'https://storage.googleapis.com/download.tensorflow.org/example_images/Green_Sea_Turtle_grazing_seagrass.jpg')\nstyle_path = tf.keras.utils.get_file(\n    'kandinsky.jpg',\n    'https://storage.googleapis.com/download.tensorflow.org/example_images/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg')\n\ndef imshow(image, title=None):\n    if len(image.shape) > 3:\n        image = tf.squeeze(image, axis=0)\n\n    plt.imshow(image)\n    if title:\n        plt.title(title)\n\ndef load_image(image_path):\n    max_dim = 512\n    image = tf.io.read_file(image_path)\n    image = tf.image.decode_image(image, channels=3)\n    image = tf.image.convert_image_dtype(image, tf.float32)\n\n    shape = tf.cast(tf.shape(image)[:-1], tf.float32)\n    long_dim = max(shape)\n    scale = max_dim / long_dim\n\n    new_shape = tf.cast(shape * scale, tf.int32)\n\n    image = tf.image.resize(image, new_shape)\n    image = image[tf.newaxis, :]\n    return image\n\nstyle_image = load_image(style_path)\ncontent_image = load_image(content_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def replace_max_by_average_pooling(model):\n\n    input_layer, *other_layers = model.layers\n    assert isinstance(input_layer, tf.keras.layers.InputLayer)\n\n    x = input_layer.output\n    for layer in other_layers:\n        if isinstance(layer, tf.keras.layers.MaxPooling2D):\n            layer = tf.keras.layers.AveragePooling2D(\n                pool_size=layer.pool_size,\n                strides=layer.strides,\n                padding=layer.padding,\n                data_format=layer.data_format,\n                name=f\"{layer.name}_av\",\n            )\n        x = layer(x)\n\n    return tf.keras.models.Model(inputs=input_layer.input, outputs=x)\n\n# If you are only interested in convolution filters. Note that by not\n# specifying the shape of top layers, the input tensor shape is (None, None, 3),\n# so you can use them for any size of images.\nvgg_model = tf.keras.applications.VGG19(\n    weights='imagenet',\n    include_top=False,\n)\n\nvgg_model.trainable = False\nvgg_model = replace_max_by_average_pooling(vgg_model)\noutputs = {\n    \"style_layers\": [vgg_model.get_layer(layer).get_output_at(1) for layer in STYLE_LAYERS],\n    \"content_layer\": vgg_model.get_layer(CONTENT_LAYER).get_output_at(1)\n}\nmodel = tf.keras.Model([vgg_model.input], outputs)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"target_content = model(content_image)[\"content_layer\"]\ntarget_style = model(style_image)[\"style_layers\"]\npredicted_outputs = {\n    \"content_layer\": target_content,\n    \"style_layers\": target_style\n}\ninput_image = tf.Variable(content_image)\nopt = tf.optimizers.Adam(learning_rate=.02, beta_1=.99, epsilon=1e-1)\n\n@tf.function\ndef compute_gram_matrix(style_features):\n    b, h, w, c = style_features.shape\n    right_side = tf.keras.backend.reshape(style_features, [b, -1, c])\n    left_side = tf.transpose(right_side, [0, 2, 1])\n    \n    gram_matrix = tf.matmul(left_side, right_side)\n    \n    # average (maybe not the most efficient)\n    gram_matrix = gram_matrix / (h * w)\n    \n    # shape [b, c, c]\n    return gram_matrix\n\n@tf.function\ndef style_transfer_loss(outputs, predicted_outputs):\n    style_weights = [tf.constant(.2, dtype=tf.float32) for i in range(5)]\n    alpha = tf.constant(1, dtype=tf.float32)\n    beta = tf.constant(1000, dtype=tf.float32)\n    #style_weights = tf.keras.backend.reshape(style_weights, [1, 1, 1, 5])\n\n    outputs[\"style_layers\"]\n    outputs[\"content_layer\"]\n\n    predicted_outputs[\"style_layers\"]\n    predicted_outputs[\"content_layer\"]\n\n    predicted_output_gram_matrices = [\n        compute_gram_matrix(layer) for layer in \n        predicted_outputs[\"style_layers\"]\n    ]\n    \n    output_gram_matrices = [\n        compute_gram_matrix(layer) for layer in \n        outputs[\"style_layers\"]\n    ]\n\n    style_loss = [\n        tf.reduce_sum(tf.keras.backend.square(output - predicted)) * weight / 4\n        for output, predicted, weight in \n        zip(output_gram_matrices, predicted_output_gram_matrices, style_weights)\n    ]\n    style_loss = tf.reduce_sum(style_loss)\n\n    content_loss = .5 * tf.keras.backend.square(\n        outputs[\"content_layer\"] - predicted_outputs[\"content_layer\"]\n    )\n    content_loss = tf.reduce_mean(content_loss)\n\n    loss = alpha * content_loss + beta * style_loss\n    return loss\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef clip_0_1(image):\n    \n    return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)\n\ndef high_pass_x_y(image):\n    x_var = image[:,:,1:,:] - image[:,:,:-1,:]\n    y_var = image[:,1:,:,:] - image[:,:-1,:,:]\n\n    return x_var, y_var\n\ndef total_variation_loss(image):\n    x_deltas, y_deltas = high_pass_x_y(image)\n    return tf.reduce_mean(x_deltas**2) + tf.reduce_mean(y_deltas**2)\n\n@tf.function\ndef train_step(input_image):\n    with tf.GradientTape() as tape:\n        output = model(input_image)\n        loss = style_transfer_loss(output, predicted_outputs)\n        loss += VARIATION_LOSS_WEIGHT * \\\n            total_variation_loss(input_image)\n    gradients = tape.gradient(loss, input_image)\n    opt.apply_gradients([(gradients, input_image)])\n    input_image.assign(clip_0_1(input_image))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_steps = 100\nnum_epochs = 10\nfor epoch in range(1, 1 + num_epochs):\n    for i in range(num_steps):\n        train_step(input_image)\n    plt.subplot(2, 5, epoch)\n    imshow(tf.identity(input_image), 'Epoch: {}'.format(epoch))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplot(1, 3, 1)\nimshow(content_image, \"Content Image\")\nplt.subplot(1, 3, 2)\nimshow(style_image, \"Style Image\")\nplt.subplot(1, 3, 3)\nimshow(input_image, \"New Image\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# How to save input_image\nfrom PIL import Image\nImage.fromarray(\n    np.array(\n        tf.image.convert_image_dtype(\n            tf.squeeze(input_image) ,tf.uint8\n        )\n    )\n).save(\"scanner_darkly_me.png\")\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}