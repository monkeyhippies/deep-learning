{
  "cells": [
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ed6e4c87bbf180631bdb83518b17483f6c29f989"
      },
      "cell_type": "markdown",
      "source": "Dueling, Double Q Network to solve simple game. Much of the code was taken from https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df\n\nAlso added coordConv to the input images per https://arxiv.org/abs/1807.03247"
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "!apt-get install -y python-opengl\n!pip install gym[atari]\n!pip install JSAnimation",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Reading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following additional packages will be installed:\n  freeglut3 libglu1-mesa\nSuggested packages:\n  python-tk python-numpy libgle3\nThe following NEW packages will be installed:\n  freeglut3 libglu1-mesa python-opengl\n0 upgraded, 3 newly installed, 0 to remove and 42 not upgraded.\nNeed to get 791 kB of archives.\nAfter this operation, 6260 kB of additional disk space will be used.\nGet:1 http://deb.debian.org/debian stretch/main amd64 freeglut3 amd64 2.8.1-3 [123 kB]\nGet:2 http://deb.debian.org/debian stretch/main amd64 libglu1-mesa amd64 9.0.0-2.1 [168 kB]\nGet:3 http://deb.debian.org/debian stretch/main amd64 python-opengl all 3.1.0+dfsg-1 [501 kB]\nFetched 791 kB in 0s (7118 kB/s)       \ndebconf: delaying package configuration, since apt-utils is not installed\nSelecting previously unselected package freeglut3:amd64.\n(Reading database ... 35423 files and directories currently installed.)\nPreparing to unpack .../freeglut3_2.8.1-3_amd64.deb ...\nUnpacking freeglut3:amd64 (2.8.1-3) ...\nSelecting previously unselected package libglu1-mesa:amd64.\nPreparing to unpack .../libglu1-mesa_9.0.0-2.1_amd64.deb ...\nUnpacking libglu1-mesa:amd64 (9.0.0-2.1) ...\nSelecting previously unselected package python-opengl.\nPreparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\nUnpacking python-opengl (3.1.0+dfsg-1) ...\nSetting up freeglut3:amd64 (2.8.1-3) ...\nProcessing triggers for libc-bin (2.24-11+deb9u3) ...\nSetting up libglu1-mesa:amd64 (9.0.0-2.1) ...\nSetting up python-opengl (3.1.0+dfsg-1) ...\nRequirement already satisfied: gym[atari] in /opt/conda/lib/python3.6/site-packages (0.11.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from gym[atari]) (1.1.0)\nRequirement already satisfied: requests>=2.0 in /opt/conda/lib/python3.6/site-packages (from gym[atari]) (2.21.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from gym[atari]) (1.12.0)\nRequirement already satisfied: pyglet>=1.2.0 in /opt/conda/lib/python3.6/site-packages (from gym[atari]) (1.3.2)\nRequirement already satisfied: numpy>=1.10.4 in /opt/conda/lib/python3.6/site-packages (from gym[atari]) (1.16.1)\nCollecting atari-py>=0.1.4; extra == \"atari\" (from gym[atari])\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/23/96ce488af867646277d91c3c9a1f328a5452240fe51df0a5fdf9211d76e0/atari_py-0.1.7-cp36-cp36m-manylinux1_x86_64.whl (2.6MB)\n\u001b[K    100% |████████████████████████████████| 2.6MB 10.2MB/s ta 0:00:01\n\u001b[?25hRequirement already satisfied: Pillow; extra == \"atari\" in /opt/conda/lib/python3.6/site-packages (from gym[atari]) (5.1.0)\nRequirement already satisfied: PyOpenGL; extra == \"atari\" in /opt/conda/lib/python3.6/site-packages (from gym[atari]) (3.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.0->gym[atari]) (2018.11.29)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.0->gym[atari]) (3.0.4)\nRequirement already satisfied: urllib3<1.25,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.0->gym[atari]) (1.22)\nRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.0->gym[atari]) (2.6)\nRequirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from pyglet>=1.2.0->gym[atari]) (0.17.1)\nInstalling collected packages: atari-py\nSuccessfully installed atari-py-0.1.7\n\u001b[33mYou are using pip version 18.1, however version 19.0.3 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\nCollecting JSAnimation\n  Downloading https://files.pythonhosted.org/packages/3c/e6/a93a578400c38a43af8b4271334ed2444b42d65580f1d6721c9fe32e9fd8/JSAnimation-0.1.tar.gz\nBuilding wheels for collected packages: JSAnimation\n  Running setup.py bdist_wheel for JSAnimation ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /tmp/.cache/pip/wheels/3c/c2/b2/b444dffc3eed9c78139288d301c4009a42c0dd061d3b62cead\nSuccessfully built JSAnimation\nInstalling collected packages: JSAnimation\nSuccessfully installed JSAnimation-0.1\n\u001b[33mYou are using pip version 18.1, however version 19.0.3 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e800e5f70f319028d06bebca0e0ff3695616138d"
      },
      "cell_type": "code",
      "source": "# Taken from https://arxiv.org/abs/1807.03247\ndef addCoords(image):\n    n_rows, n_columns, _ = image.shape\n    rows = np.matmul(np.reshape(np.array(range(n_rows)), [n_rows, 1]), np.ones([1, n_columns]))\n    columns = np.matmul(np.ones([n_rows, 1]), np.reshape(np.array(range(n_columns)), [1, n_columns]))\n    rows = (rows - (n_rows - 1) / 2) / ((n_rows -1 ) / 2)\n    rows = np.expand_dims(rows, 2)\n    columns = (columns - (n_columns - 1) / 2) / ((n_columns - 1) / 2)\n    columns = np.expand_dims(columns, 2)\n\n    return np.concatenate([image, rows, columns], axis=2)\n",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3fe5d8fd1929626f75a6739862d35fe3ce3fd9bd"
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport random\nimport itertools\nimport scipy.misc\nimport matplotlib.pyplot as plt\n\n\nclass gameOb():\n    def __init__(self,coordinates,size,intensity,channel,reward,name):\n        self.x = coordinates[0]\n        self.y = coordinates[1]\n        self.size = size\n        self.intensity = intensity\n        self.channel = channel\n        self.reward = reward\n        self.name = name\n        \nclass GameEnv():\n    class ActionSpace(object):\n        \n        def __init__(self, n):\n            self.n = n\n    def __init__(self,partial,size):\n        self.sizeX = size\n        self.sizeY = size\n        self.actions = 4\n        self.action_space = object()\n        self.action_space = self.ActionSpace(self.actions)\n        self.objects = []\n        self.partial = partial\n        a = self.reset()\n\n        \n        \n    def reset(self):\n        self.objects = []\n        hero = gameOb(self.newPosition(),1,1,2,None,'hero')\n        self.objects.append(hero)\n        bug = gameOb(self.newPosition(),1,1,1,1,'goal')\n        self.objects.append(bug)\n        hole = gameOb(self.newPosition(),1,1,0,-1,'fire')\n        self.objects.append(hole)\n        bug2 = gameOb(self.newPosition(),1,1,1,1,'goal')\n        self.objects.append(bug2)\n        hole2 = gameOb(self.newPosition(),1,1,0,-1,'fire')\n        self.objects.append(hole2)\n        bug3 = gameOb(self.newPosition(),1,1,1,1,'goal')\n        self.objects.append(bug3)\n        bug4 = gameOb(self.newPosition(),1,1,1,1,'goal')\n        self.objects.append(bug4)\n        state = self.renderEnv()\n        self.state = state\n        return state\n\n    def moveChar(self,direction):\n        # 0 - up, 1 - down, 2 - left, 3 - right\n        hero = self.objects[0]\n        heroX = hero.x\n        heroY = hero.y\n        penalize = 0\n        if direction == 0 and hero.y >= 1:\n            hero.y -= 1\n        if direction == 1 and hero.y <= self.sizeY-2:\n            hero.y += 1\n        if direction == 2 and hero.x >= 1:\n            hero.x -= 1\n        if direction == 3 and hero.x <= self.sizeX-2:\n            hero.x += 1     \n        if hero.x == heroX and hero.y == heroY:\n            penalize = 0\n        self.objects[0] = hero\n        return penalize\n    \n    def newPosition(self):\n        iterables = [ range(self.sizeX), range(self.sizeY)]\n        points = []\n        for t in itertools.product(*iterables):\n            points.append(t)\n        currentPositions = []\n        for objectA in self.objects:\n            if (objectA.x,objectA.y) not in currentPositions:\n                currentPositions.append((objectA.x,objectA.y))\n        for pos in currentPositions:\n            points.remove(pos)\n        location = np.random.choice(range(len(points)),replace=False)\n        return points[location]\n\n    def checkGoal(self):\n        others = []\n        for obj in self.objects:\n            if obj.name == 'hero':\n                hero = obj\n            else:\n                others.append(obj)\n        ended = False\n        for other in others:\n            if hero.x == other.x and hero.y == other.y:\n                self.objects.remove(other)\n                if other.reward == 1:\n                    self.objects.append(gameOb(self.newPosition(),1,1,1,1,'goal'))\n                else: \n                    self.objects.append(gameOb(self.newPosition(),1,1,0,-1,'fire'))\n                return other.reward,False\n        if ended == False:\n            return 0.0,False\n\n    def renderEnv(self):\n        #a = np.zeros([self.sizeY,self.sizeX,3])\n        a = np.ones([self.sizeY+2,self.sizeX+2,3])\n        a[1:-1,1:-1,:] = 0\n        hero = None\n        for item in self.objects:\n            a[item.y+1:item.y+item.size+1,item.x+1:item.x+item.size+1,item.channel] = item.intensity\n            if item.name == 'hero':\n                hero = item\n        if self.partial == True:\n            a = a[hero.y:hero.y+3,hero.x:hero.x+3,:]\n        b = scipy.misc.imresize(a[:,:,0],[84,84,1],interp='nearest')\n        c = scipy.misc.imresize(a[:,:,1],[84,84,1],interp='nearest')\n        d = scipy.misc.imresize(a[:,:,2],[84,84,1],interp='nearest')\n        a = np.stack([b,c,d],axis=2)\n        return addCoords(a)\n\n    def step(self,action):\n        penalty = self.moveChar(action)\n        reward,done = self.checkGoal()\n        state = self.renderEnv()\n        if reward == None:\n            print(done)\n            print(reward)\n            print(penalty)\n            return state,(reward+penalty),done, None\n        else:\n            return state,(reward+penalty),done, None",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "997362d5250fd1f2ae2f3665ffb4e4fbc888a73a"
      },
      "cell_type": "code",
      "source": "class QNetwork(object):\n    \n    def __init__(self, a_size, h_size, scope, input_dims):\n        \n        self.a_size = a_size\n        self.h_size = h_size\n        self.scope = scope\n        self.input_dims = list(input_dims)\n        self.trainer = tf.train.AdamOptimizer(learning_rate=.0001)\n\n        with tf.variable_scope(self.scope):\n            self.input = tf.placeholder(\n                shape=[None] + self.input_dims,\n                dtype=tf.float32\n            )\n            \n            self.conv1 = slim.conv2d(\n                self.input,\n                kernel_size=[3, 3],\n                num_outputs=32,\n                stride=[1, 1],\n                padding=\"VALID\"\n            )\n\n            self.conv2 = slim.conv2d(\n                self.conv1,\n                kernel_size=[4, 4],\n                num_outputs=64,\n                stride=[2, 2],\n                padding=\"VALID\"\n            )\n            \n            self.conv3 = slim.conv2d(\n                self.conv2,\n                kernel_size=[8, 8],\n                num_outputs=64,\n                stride=[4, 4],\n                padding=\"VALID\"\n            )\n            \n            \n            self.conv4 = slim.conv2d(\n                self.conv3,\n                kernel_size=[7, 7],\n                num_outputs=h_size,\n                stride=[1, 1],\n                padding=\"VALID\"\n            )\n            \n            self.A_stream, self.V_stream = tf.split(\n                self.conv4,\n                2,\n                axis=3\n            )\n\n            \"\"\"\n            self.advantage = slim.fully_connected(\n                tf.layers.flatten(self.A_stream),\n                self.a_size\n            )\n            \n            self.value = slim.fully_connected(\n                tf.layers.flatten(self.V_stream),\n                1\n            )\n            \"\"\"\n            self.streamA = tf.layers.flatten(self.A_stream)\n            self.streamV = tf.layers.flatten(self.V_stream)\n            xavier_init = tf.contrib.layers.xavier_initializer()\n            self.AW = tf.Variable(xavier_init([self.h_size * 9 // 2, self.a_size]))\n            self.VW = tf.Variable(xavier_init([self.h_size * 9 // 2, 1]))\n            self.advantage = tf.matmul(self.streamA, self.AW)\n            self.value = tf.matmul(self.streamV, self.VW)\n\n            self.Q = (self.advantage - tf.reduce_mean(self.advantage, 1, keep_dims=True)) + self.value\n            self.prediction = tf.argmax(self.Q, 1)\n            \n            self.target_Q = tf.placeholder(\n                shape=[None],\n                dtype=tf.float32\n            )\n            \n            self.action = tf.placeholder(\n                shape=[None],\n                dtype=tf.int32\n            )\n            \n            action_onehot = tf.one_hot(self.action, self.a_size)\n            self.loss = tf.reduce_mean(tf.square(\n                tf.reduce_sum(\n                    action_onehot * self.Q, 1\n                ) - self.target_Q\n            ))\n\n            self.minimize = self.trainer.minimize(self.loss)\n            \n            ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1e26ba430759c3d824369b06951ca09c3e627f93"
      },
      "cell_type": "code",
      "source": "def update_ops(from_scope, to_scope, tau):\n    \n    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n    \n    ops = []\n    for from_var, to_var in zip(from_vars, to_vars):\n        op = to_var.assign(to_var.value() * (1 - tau) + from_var.value() * tau)\n        ops.append(op)\n\n    return ops\n\nclass ExperienceBuffer(object):\n    \n    def __init__(self, buffer_size=50000):\n        \n        self.buffer_size = buffer_size\n        self._buffer = []\n        \n    def reset(self):\n        \n        self._buffer = []\n \n    def sample(self, size):\n        \n        return random.sample(self._buffer, size)\n\n    def add(self, experiences):\n        \n        e_length = len(experiences)\n        current_size = len(self._buffer)\n        if e_length + current_size > self.buffer_size:\n            self._buffer = self._buffer[e_length + current_size - self.buffer_size: ] \\\n                + experiences\n        else:\n            self._buffer.extend(experiences)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3bf4f6d211f1aa776049463b051d8c759c32ae47"
      },
      "cell_type": "code",
      "source": "class Worker(object):\n    \n    MAX_EPISODE_LENGTH = 50\n    PRE_TRAIN_STEPS = 10000\n    BASE_E = .1\n    EXTRA_E = 1 - BASE_E\n    TRAIN_STEPS = 15000\n    BATCH_SIZE = 32\n    GAMMA = .99\n    TAU = .001\n\n    def __init__(self, sess, number_episodes=10000):\n        \n        self.env = GameEnv(partial=False, size=5)\n        self.sess = sess\n        self.buffer = ExperienceBuffer()\n        self.number_episodes = number_episodes\n        input_dims = self.env.reset().shape\n\n        self.main_network = QNetwork(\n            a_size=self.env.action_space.n,\n            h_size=512,\n            scope=\"main\",\n            input_dims=input_dims\n        )\n        \n        self.target_network = QNetwork(\n            a_size=self.env.action_space.n,\n            h_size=512,\n            scope=\"target\",\n            input_dims=input_dims\n        )\n        \n        self.update_target_vars = update_ops(\"main\", \"target\", self.TAU)\n\n    def run(self):\n\n        global_step = 0\n        episode_rewards = []\n        e = None\n        for i in range(self.number_episodes):\n            if global_step % 250 == 0:\n                print (\"Episode: {} Reward: {}\".format(i, np.mean(episode_rewards[-25: ])))\n            state = self.env.reset()\n            episode_reward = 0\n            for step in range(self.MAX_EPISODE_LENGTH):\n                e = 1 - self.EXTRA_E * np.max(\n                    [np.min([self.TRAIN_STEPS + self.PRE_TRAIN_STEPS, global_step]) - self.PRE_TRAIN_STEPS, 0]\n                ) / self.TRAIN_STEPS\n                global_step += 1\n                Q, prediction = self.sess.run(\n                    [\n                        self.main_network.Q,\n                        self.main_network.prediction\n                    ],\n                    feed_dict={\n                        self.main_network.input: [state]\n                    }\n                )\n\n                if np.random.rand(1) < e:\n                    action = np.random.randint(self.env.action_space.n)\n                else:\n                    action = prediction\n                \n                state1, reward, done, _ = self.env.step(action)\n                self.buffer.add([(state, action, state1, reward, done)])\n\n                episode_reward += reward\n                state = state1\n                if global_step % 4 == 0 and global_step > self.PRE_TRAIN_STEPS:\n                    experiences = self.buffer.sample(self.BATCH_SIZE)\n                    states = [exp[0] for exp in experiences]\n                    actions = [exp[1] for exp in experiences]\n                    next_states = [exp[2] for exp in experiences]\n                    rewards = np.array([exp[3] for exp in experiences])\n                    dones = np.array([exp[4] for exp in experiences])\n\n                    next_actions = self.sess.run(\n                        self.main_network.prediction,\n                        feed_dict={\n                            self.main_network.input: next_states\n                        }\n                    )\n                    \n                    next_qs = self.sess.run(\n                        self.target_network.Q,\n                        feed_dict={\n                            self.target_network.input: next_states\n                        }\n                    )\n                    \n                    target_qs = rewards + self.GAMMA * (1.0 - dones) * next_qs[range(self.BATCH_SIZE), next_actions]\n                    \n                    self.sess.run(\n                        [self.main_network.minimize],\n                        feed_dict={\n                            self.main_network.input: states,\n                            self.main_network.target_Q: target_qs,\n                            self.main_network.action: actions\n                        }\n                    )\n                    \n                    self.sess.run(\n                        self.update_target_vars\n                    )\n\n                if done:\n                    break\n            episode_rewards.append(episode_reward)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "745f3a53187b57d643e200fd5262aef41e2c9dc6"
      },
      "cell_type": "code",
      "source": "tf.reset_default_graph()\n\nwith tf.Session() as sess:\n    worker = Worker(sess=sess)\n    sess.run(tf.global_variables_initializer())\n    worker.run()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c04a0c95b55b333b54f94a4affe94ace837573c5"
      },
      "cell_type": "code",
      "source": "tf.layers",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ee9e7107d812e54ff6902a602e37bca83581d587"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}