{
  "cells": [
    {
      "metadata": {
        "_uuid": "b730f24122129a77a010fd284dc222618658ff9d"
      },
      "cell_type": "markdown",
      "source": "A3C Algorithm to solve a simple game. Not currently converging to a solution.\nLots of the code was taken from this repo https://github.com/awjuliani/DeepRL-Agents"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "18b0031391c2fea165185f5184c616b2154c5cb8"
      },
      "cell_type": "code",
      "source": "!apt-get install -y python-opengl\n!pip install gym[atari]\n!pip install JSAnimation",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport random\nimport itertools\nimport scipy.misc\nimport matplotlib.pyplot as plt\n\n\nclass gameOb():\n    def __init__(self,coordinates,size,intensity,channel,reward,name):\n        self.x = coordinates[0]\n        self.y = coordinates[1]\n        self.size = size\n        self.intensity = intensity\n        self.channel = channel\n        self.reward = reward\n        self.name = name\n        \nclass gameEnv():\n    class ActionSpace(object):\n        \n        def __init__(self, n):\n            self.n = n\n    def __init__(self,partial,size):\n        self.sizeX = size\n        self.sizeY = size\n        self.actions = 4\n        self.action_space = object()\n        self.action_space = self.ActionSpace(self.actions)\n        self.objects = []\n        self.partial = partial\n        a = self.reset()\n        plt.imshow(a,interpolation=\"nearest\")\n        \n        \n    def reset(self):\n        self.objects = []\n        hero = gameOb(self.newPosition(),1,1,2,None,'hero')\n        self.objects.append(hero)\n        bug = gameOb(self.newPosition(),1,1,1,1,'goal')\n        self.objects.append(bug)\n        hole = gameOb(self.newPosition(),1,1,0,-1,'fire')\n        self.objects.append(hole)\n        bug2 = gameOb(self.newPosition(),1,1,1,1,'goal')\n        self.objects.append(bug2)\n        hole2 = gameOb(self.newPosition(),1,1,0,-1,'fire')\n        self.objects.append(hole2)\n        bug3 = gameOb(self.newPosition(),1,1,1,1,'goal')\n        self.objects.append(bug3)\n        bug4 = gameOb(self.newPosition(),1,1,1,1,'goal')\n        self.objects.append(bug4)\n        state = self.renderEnv()\n        self.state = state\n        return state\n\n    def moveChar(self,direction):\n        # 0 - up, 1 - down, 2 - left, 3 - right\n        hero = self.objects[0]\n        heroX = hero.x\n        heroY = hero.y\n        penalize = -0.05\n        if direction == 0 and hero.y >= 1:\n            hero.y -= 1\n        if direction == 1 and hero.y <= self.sizeY-2:\n            hero.y += 1\n        if direction == 2 and hero.x >= 1:\n            hero.x -= 1\n        if direction == 3 and hero.x <= self.sizeX-2:\n            hero.x += 1     \n        if hero.x == heroX and hero.y == heroY:\n            penalize = -0.05\n        self.objects[0] = hero\n        return penalize\n    \n    def newPosition(self):\n        iterables = [ range(self.sizeX), range(self.sizeY)]\n        points = []\n        for t in itertools.product(*iterables):\n            points.append(t)\n        currentPositions = []\n        for objectA in self.objects:\n            if (objectA.x,objectA.y) not in currentPositions:\n                currentPositions.append((objectA.x,objectA.y))\n        for pos in currentPositions:\n            points.remove(pos)\n        location = np.random.choice(range(len(points)),replace=False)\n        return points[location]\n\n    def checkGoal(self):\n        others = []\n        for obj in self.objects:\n            if obj.name == 'hero':\n                hero = obj\n            else:\n                others.append(obj)\n        ended = False\n        for other in others:\n            if hero.x == other.x and hero.y == other.y:\n                self.objects.remove(other)\n                if other.reward == 1:\n                    self.objects.append(gameOb(self.newPosition(),1,1,1,1,'goal'))\n                else: \n                    self.objects.append(gameOb(self.newPosition(),1,1,0,-1,'fire'))\n                return other.reward,False\n        if ended == False:\n            return 0.0,False\n\n    def renderEnv(self):\n        #a = np.zeros([self.sizeY,self.sizeX,3])\n        a = np.ones([self.sizeY+2,self.sizeX+2,3])\n        a[1:-1,1:-1,:] = 0\n        hero = None\n        for item in self.objects:\n            a[item.y+1:item.y+item.size+1,item.x+1:item.x+item.size+1,item.channel] = item.intensity\n            if item.name == 'hero':\n                hero = item\n        if self.partial == True:\n            a = a[hero.y:hero.y+3,hero.x:hero.x+3,:]\n        b = scipy.misc.imresize(a[:,:,0],[84,84,1],interp='nearest')\n        c = scipy.misc.imresize(a[:,:,1],[84,84,1],interp='nearest')\n        d = scipy.misc.imresize(a[:,:,2],[84,84,1],interp='nearest')\n        a = np.stack([b,c,d],axis=2)\n        return a\n\n    def step(self,action):\n        penalty = self.moveChar(action)\n        reward,done = self.checkGoal()\n        state = self.renderEnv()\n        if reward == None:\n            print(done)\n            print(reward)\n            print(penalty)\n            return state,(reward+penalty),done, None\n        else:\n            return state,(reward+penalty),done, None\n        \nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport gym\nimport random\nimport multiprocessing\nimport threading\nfrom collections import namedtuple\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Imports specifically so we can render outputs in Jupyter.\nfrom JSAnimation.IPython_display import display_animation\nfrom matplotlib import animation\nfrom IPython.display import display\n\ndef display_frames_as_gif(frames):\n    \"\"\"\n    Displays a list of frames as a gif, with controls\n    \"\"\"\n    #plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n    patch = plt.imshow(frames[0])\n    plt.axis('off')\n\n    def animate(i):\n        patch.set_data(frames[i])\n\n    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n    display(display_animation(anim, default_mode='loop'))\n\n\ndef wrapper(func):\n    def wrapped(*args, **kwargs):\n        if args[0] == 'test':\n            env = gameEnv(partial=False, size=9)\n        else:\n            env = func(*args, **kwargs)\n\n        return env\n    return wrapped\n\ngym.make = wrapper(gym.make)\n\"\"\"\nenv = gym.make('Assault-v0')\nobservation = env.reset()\naction_size = env.action_space.n\nframes = []\nfor i in range(100):\n    observation, reward, done, info = env.step(random.randint(0, 6))\n    frames.append(env.render(mode='rgb_array'))\n    if done:\n        break\ndisplay_frames_as_gif(frames)\n\"\"\"\n\ndef update_vars(from_scope, to_scope):\n    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n    ops = []\n    for to_var, from_var in zip(to_vars, from_vars):\n        op = to_var.assign(from_var)\n        ops.append(op)\n        \n    return ops\n\nclass Network(object):\n\n    def __init__(self, sess, input_dims, action_size, hidden_size, scope, trainer):\n        self.sess = sess\n        self.input_dims = input_dims\n        self.action_size = action_size\n        self.hidden_size = hidden_size\n        self.scope = scope\n        self.trainer = trainer\n        self.calculator = FeedCalculator()\n\n\n        with tf.variable_scope(scope):\n            self.input = tf.placeholder(\n                shape=[None] + list(self.input_dims),\n                dtype=tf.float32\n            )\n            \n            self.input /= 255.0\n            self.conv1 = slim.conv2d(\n                inputs=self.input,\n                num_outputs=16,\n                kernel_size=[4, 4],\n                stride=[2, 2],\n                activation_fn=tf.nn.elu,\n                padding='VALID'\n            )\n\n            self.conv2 = slim.conv2d(\n                inputs=self.conv1,\n                num_outputs=32,\n                kernel_size=[8, 8],\n                stride=[4, 4],\n                activation_fn=tf.nn.elu,\n                padding='VALID'\n            )\n            \n            self.connected = slim.fully_connected(\n                tf.layers.flatten(self.conv2),\n                self.hidden_size,\n                activation_fn=tf.nn.elu\n            )\n\n            lstm_cell = tf.contrib.rnn.BasicLSTMCell(self.hidden_size, state_is_tuple=True)\n            #self.rnn_zero_state = lstm_cell.zero_state(1, dtype=tf.float32)\n            self.rnn_zero_state = (\n                np.zeros([1, self.hidden_size], dtype=\"float32\"),\n                np.zeros([1, self.hidden_size], dtype=\"float32\")\n            )\n            \n            self.c_in = tf.placeholder(\n                shape=[1, self.hidden_size],\n                dtype=tf.float32\n            )\n            \n            self.h_in = tf.placeholder(\n                shape=[1, self.hidden_size],\n                dtype=tf.float32\n            )\n            \n            self.state_in = tf.nn.rnn_cell.LSTMStateTuple(self.c_in, self.h_in)\n    \n            output, state = tf.nn.dynamic_rnn(\n                cell=lstm_cell,\n                inputs=tf.expand_dims(self.connected,[0]),\n                initial_state=self.state_in,\n                time_major=False,\n            )\n\n            self.rnn_state = state\n\n            output = tf.reshape(output, shape=[-1, self.hidden_size])\n            #output = tf.reshape(self.connected, shape=[-1, self.hidden_size])\n            self.policy = slim.fully_connected(\n                output,\n                self.action_size,\n                activation_fn=tf.nn.softmax\n            )\n\n            self.value = slim.fully_connected(\n                output,\n                1\n            )\n\n            self.advantage = tf.placeholder(shape=[None], dtype=tf.float32)\n            self.action = tf.placeholder(shape=[None], dtype=tf.int32)\n            self.target_v = tf.placeholder(shape=[None], dtype=tf.float32)\n            self.action_onehot = tf.one_hot(self.action, self.action_size)\n            \n            responsible_outputs = tf.reduce_sum(\n                self.policy * self.action_onehot,\n                [1]\n            )\n            \n            self.policy_loss = - tf.reduce_sum(\n                tf.log(responsible_outputs) * self.advantage,\n            )\n            \n            self.value_loss = tf.reduce_sum(\n                tf.square(self.value - self.target_v),\n            )\n            self.entropy_loss = - tf.reduce_sum(\n                tf.log(self.policy) * self.policy,\n            )\n\n            self.loss = self.policy_loss + .25 * self.value_loss - .01 * self.entropy_loss\n            local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, self.scope)\n            grads = tf.gradients(self.loss, local_vars)\n            grads, norms = tf.clip_by_global_norm(grads, 40.0)\n            global_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'global')\n            self.minimize = self.trainer.apply_gradients(zip(grads, global_vars))\n            self.update_local_ops = update_vars(\"global\", self.scope)\n\n    def runUpdateVariables(self):\n\n        return self.sess.run(self.update_local_ops)\n\n    def runPolicyValueState(self, inputs, initial_rnn_state):\n        \n        action_dist, value, state = sess.run(\n            [self.policy, self.value, self.rnn_state],\n            feed_dict={\n                self.input: inputs,\n                self.c_in: initial_rnn_state[0],\n                self.h_in: initial_rnn_state[1]\n            }\n        )\n        \n        return action_dist, value, state\n        \n    def runMinimize(self, experiences, initial_rnn_state, next_value):\n        \n        advantages, target_values = \\\n            self.calculator.advantages_and_target_values(\n                experiences, next_value\n            )\n\n        inputs = np.array([e.state for e in experiences])\n        actions = np.array([e.action for e in experiences])\n        \n        _ = sess.run(\n            [self.minimize],\n            feed_dict={\n                self.input: inputs,\n                self.c_in: initial_rnn_state[0],\n                self.h_in: initial_rnn_state[1],\n                self.advantage: advantages,\n                self.target_v: target_values,\n                self.action: actions\n            }\n        )\n\nimport scipy.signal\n\nclass FeedCalculator(object):\n    \n    def __init__(self, gamma=.99):\n        \n        self.gamma = gamma\n    \n    def discount(self, array):\n        \n        return scipy.signal.lfilter(\n            [1],\n            [1, -self.gamma],\n            array[: : -1],\n            axis=0\n        )[: : -1]\n\n    def advantages_and_target_values(self, experiences, next_value):\n        \n        rewards = np.array([e.reward for e in experiences] + [next_value])\n        values = np.array([e.value for e in experiences] + [next_value])\n        target_values = self.discount(rewards)[:-1]\n        advantages = rewards[:-1] + self.gamma * values[1: ] - values[: -1]\n        return advantages, target_values\n        \nclass BufferFullException(Exception):\n    pass\n\nExperience = namedtuple(\n    \"Experience\",\n    \"state action reward next_state done value\"\n)\n\n\nclass ExperienceBuffer(object):\n    \n    def __init__(self, size):\n        \n        self._buffer = []\n        self._size = size\n\n    @property\n    def is_full(self):\n        \n        return len(self._buffer) == self._size\n\n    @property\n    def experiences(self):\n        \n        return self._buffer\n\n    def reset(self):\n        \n        self._buffer = []\n\n    def add(self, experience):\n        \n        if len(self._buffer) >= self._size:\n            raise BufferFullException(\n                \"Buffer reached max size of {}\".format(self._size)\n            )\n            \n        self._buffer.append(experience)\n        \nclass Worker(threading.Thread):\n    \n    MAX_EPISODE_LENGTH = 300\n    BUFFER_SIZE = 50\n    def __init__(self, name, env_name, sess, trainer, hidden_size, global_network):\n        \n        super(Worker, self).__init__()\n        self.name = \"worker_\" + str(name)\n        self.env = gym.make(env_name)\n        self.global_network = global_network\n        self.network = Network(\n            scope=self.name,\n            sess=sess,\n            input_dims=self.env.reset().shape,\n            action_size=self.env.action_space.n,\n            trainer=trainer,\n            hidden_size=hidden_size\n        )\n        self.buffer = ExperienceBuffer(self.BUFFER_SIZE)\n        self.initial_rnn_state = None\n        self.episode_count = 0\n        self.episode_rewards = []\n        self.running_rewards = 0\n\n    def print_rewards(self):\n        print(\"worker: {} episode: {} reward: {}\".format(\n            self.name,\n            self.episode_count,\n            np.mean(self.episode_rewards[-25:])\n        ))\n\n    def reset_buffer(self):\n\n        self.buffer.reset()\n\n    def run(self):\n        self.work()\n\n    def work(self):\n        \n        for j in range(5000):\n            self.episode_count = j\n            if self.episode_count % 1 == 0:\n                self.network.runUpdateVariables()\n            if j != 0:\n                self.episode_rewards.append(self.running_rewards)\n                self.running_rewards = 0\n            if self.episode_count % 25 == 0:\n                self.print_rewards()\n\n            state = self.env.reset()\n            self.initial_rnn_state = self.network.rnn_zero_state\n            self.running_rewards = 0\n            for i in range(self.MAX_EPISODE_LENGTH):\n                policy, value, rnn_state = self.network.runPolicyValueState(\n                    inputs=[state],\n                    initial_rnn_state = self.initial_rnn_state\n                )\n                policy = policy[0]\n                value = value[0][0]\n                action = np.random.choice(policy, p=policy)\n                action = np.argmax(action == policy)\n                state1, reward, done, _ = self.env.step(action)\n                self.running_rewards += reward\n                experience = Experience(\n                    state,\n                    action,\n                    reward,\n                    state1,\n                    done,\n                    value\n                )\n\n                self.buffer.add(experience)\n\n                if done == True:\n                    self.network.runMinimize(\n                        self.buffer.experiences,\n                        self.initial_rnn_state,\n                        next_value=0.0\n                    )\n                    self.reset_buffer()\n                    break\n                if self.buffer.is_full:\n                    \n                    _, next_value, _ = self.network.runPolicyValueState(\n                        inputs=[state1],\n                        initial_rnn_state=rnn_state\n                    )\n                    self.network.runMinimize(\n                        self.buffer.experiences,\n                        self.initial_rnn_state,\n                        next_value\n                    )\n                    self.reset_buffer()\n                    self.initial_rnn_state = rnn_state\n\nhidden_size = 256\ntrainer = tf.train.AdamOptimizer(learning_rate=.0001)\nn_workers = multiprocessing.cpu_count()\nworkers = []\nworker_threads = []\n#env_name = \"Pong-v0\"\nenv_name = \"test\"\n#env_name = \"Assault-v0\"\nenv = gym.make(env_name)\n\ntf.reset_default_graph()\n\nwith tf.Session() as sess:\n    coord = tf.train.Coordinator()\n    global_network = Network(\n        scope=\"global\",\n        sess=sess,\n        input_dims=env.reset().shape,\n        action_size=env.action_space.n,\n        trainer=trainer,\n        hidden_size=hidden_size\n    )\n\n\n    for i in range(n_workers):\n        w = Worker(\n            name=i,\n            env_name=env_name,\n            sess=sess,\n            trainer=trainer,\n            hidden_size=hidden_size,\n            global_network=global_network\n        )\n\n\n        workers.append(w)\n\n    sess.run(tf.global_variables_initializer())\n    for w in workers:\n        w.start()\n        \n    coord.join(workers)\n\n\n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}